#!/usr/bin/env python3
"""
Self-updating portfolio data fetcher

Schedule this to run weekly on Wednesdays with cron:
0 9 * * 3 /usr/bin/python3 /path/to/update.py

To change the frequency, modify the cron schedule or the DAYS_BACK variable below
"""

import os
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
import requests
from dotenv import load_dotenv

# Configuration
SCRIPT_DIR = Path(__file__).parent.absolute()
DAYS_BACK = 3  # How many days back to fetch data (change this to adjust time window)
LOG_FILE = SCRIPT_DIR / "update.log"
DATA_FILE = SCRIPT_DIR / "data.json"
RAW_LINKEDIN_FILE = SCRIPT_DIR / "linkedin_raw.json"

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_env():
    """Load environment variables from .env file"""
    env_file = SCRIPT_DIR / ".env"
    if not env_file.exists():
        logger.error(".env file not found")
        raise FileNotFoundError(".env file not found")

    load_dotenv(env_file)
    logger.info("Environment variables loaded")


def fetch_linkedin_data():
    """Fetch LinkedIn activity data"""
    token = os.getenv("LINKEDIN_ACCESS_TOKEN")
    if not token:
        logger.error("LINKEDIN_ACCESS_TOKEN not found in .env")
        raise ValueError("LINKEDIN_ACCESS_TOKEN not found")

    # Calculate start time (DAYS_BACK days ago in milliseconds)
    start_time = datetime.now() - timedelta(days=DAYS_BACK)
    start_time_ms = int(start_time.timestamp() * 1000)

    logger.info(f"Fetching LinkedIn data from last {DAYS_BACK} days...")

    url = "https://api.linkedin.com/rest/memberChangeLogs"
    headers = {
        "Authorization": f"Bearer {token}",
        "Linkedin-Version": "202312",
        "X-Restli-Protocol-Version": "2.0.0"
    }
    params = {
        "q": "memberAndApplication",
        "startTime": start_time_ms,
        "count": 50
    }

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Save raw response for debugging
        with open(RAW_LINKEDIN_FILE, 'w') as f:
            json.dump(data, f, indent=2)
        logger.info(f"Raw LinkedIn response saved to {RAW_LINKEDIN_FILE}")

        logger.info(f"LinkedIn data fetched successfully ({len(data.get('elements', []))} items)")
        return data

    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to fetch LinkedIn data: {e}")
        return {"elements": []}


def parse_linkedin_posts(linkedin_data):
    """
    Parse LinkedIn data and extract posts

    TODO: Implement proper parsing based on actual API response structure
    TODO: PLACEHOLDER - Detect partner's reposts and handle separately
    """
    elements = linkedin_data.get("elements", [])

    if not elements:
        logger.warning("No LinkedIn elements found")
        return []

    posts = []
    # TODO: Parse actual LinkedIn data structure
    # This is a placeholder - adjust based on your actual API response
    for element in elements:
        post = {
            "date": datetime.now().isoformat(),
            "content": "LinkedIn post content parsing to be implemented",
            "summary": "Summary to be generated by LLM"
        }
        posts.append(post)

    logger.info(f"Parsed {len(posts)} LinkedIn posts")
    return posts[:5]  # Limit to 5 most recent


def summarize_with_llm(posts):
    """
    Send posts to LLM for summarization

    TODO: PLACEHOLDER - LLM Integration
    Implement this function when ready to add LLM summarization.

    Options:
    1. OpenAI (GPT-4):
        - pip install openai
        - Use OPENAI_API_KEY from .env

    2. Anthropic (Claude):
        - pip install anthropic
        - Use ANTHROPIC_API_KEY from .env

    3. Local LLM (Ollama):
        - No API key needed
        - Use requests to call local endpoint

    Example implementation:
    ```python
    import anthropic
    client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

    posts_text = "\n\n".join([p["content"] for p in posts])
    message = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"Summarize these LinkedIn posts concisely:\\n\\n{posts_text}"
        }]
    )

    for post in posts:
        post["summary"] = message.content[0].text
    ```
    """
    logger.info("LLM summarization not yet implemented (placeholder)")
    return posts


def fetch_github_projects():
    """
    Fetch recent GitHub repository data

    TODO: PLACEHOLDER - GitHub Integration
    Implement when GitHub username/token is provided

    Example implementation:
    ```python
    token = os.getenv("GITHUB_TOKEN")  # Optional, for higher rate limits
    username = os.getenv("PORTFOLIO_GITHUB")

    headers = {"Authorization": f"token {token}"} if token else {}
    response = requests.get(
        f"https://api.github.com/users/{username}/repos",
        headers=headers,
        params={"sort": "pushed", "per_page": 4}
    )

    projects = []
    for repo in response.json():
        projects.append({
            "name": repo["name"],
            "description": repo["description"] or "No description",
            "last_push": repo["pushed_at"]
        })
    return projects
    ```
    """
    logger.info("Using placeholder data for projects (GitHub integration pending)")
    return [
        {
            "name": "Project 1",
            "description": "Awaiting GitHub integration",
            "last_push": datetime.now().isoformat()
        },
        {
            "name": "Project 2",
            "description": "Awaiting GitHub integration",
            "last_push": datetime.now().isoformat()
        }
    ]


def generate_data_json(posts, projects):
    """Generate the data.json file for the website"""
    data = {
        "name": os.getenv("PORTFOLIO_NAME", "Your Name"),
        "title": os.getenv("PORTFOLIO_TITLE", "Your Title"),
        "linkedin_posts": posts,
        "projects": projects,
        "contact": {
            "email": os.getenv("PORTFOLIO_EMAIL", "your.email@example.com"),
            "github": os.getenv("PORTFOLIO_GITHUB", "yourusername"),
            "linkedin": os.getenv("PORTFOLIO_LINKEDIN", "https://linkedin.com/in/yourprofile")
        },
        "last_updated": datetime.now().isoformat()
    }

    with open(DATA_FILE, 'w') as f:
        json.dump(data, f, indent=2)

    logger.info(f"data.json updated successfully at {DATA_FILE}")


def main():
    """Main execution flow"""
    logger.info("=" * 50)
    logger.info("Starting portfolio update")
    logger.info("=" * 50)

    try:
        # Load environment variables
        load_env()

        # Fetch LinkedIn data
        linkedin_data = fetch_linkedin_data()

        # Parse LinkedIn posts
        posts = parse_linkedin_posts(linkedin_data)

        # TODO: Summarize with LLM (placeholder)
        posts = summarize_with_llm(posts)

        # TODO: Fetch GitHub projects (placeholder)
        projects = fetch_github_projects()

        # Generate data.json
        generate_data_json(posts, projects)

        logger.info("=" * 50)
        logger.info("Portfolio update complete")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"Portfolio update failed: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
